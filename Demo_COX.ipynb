{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from tqdm import tqdm\n",
    "from opacus.accountants import RDPAccountant\n",
    "\n",
    "from load_survival_data import load_seer\n",
    "from utils_surv import (\n",
    "    bootstrap_cindex_ci,\n",
    "    stratify_syn_pred,\n",
    "    stratify_real_pred,\n",
    "    SelectiveAccountant,\n",
    "    evaluate_coxph,\n",
    "    coxph_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 40\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Load and Preprocess Data\n",
    "# ----------------------------- #\n",
    "TRAIN, VAL, TEST, _ = load_seer()\n",
    "X_train, (dur_train, evt_train) = TRAIN\n",
    "X_val, (dur_val, evt_val) = VAL\n",
    "X_test, (dur_test, evt_test) = TEST\n",
    "\n",
    "# Convert months to years\n",
    "dur_train /= 12\n",
    "dur_val /= 12\n",
    "dur_test /= 12\n",
    "\n",
    "# Scale durations robustly (optional, useful for numerical stability)\n",
    "time_median = np.median(dur_train[evt_train == 1])\n",
    "time_iqr = np.percentile(dur_train[evt_train == 1], 75) - np.percentile(dur_train[evt_train == 1], 25)\n",
    "time_scale = time_iqr if time_iqr > 0 else time_median\n",
    "\n",
    "valid_idx = (~np.isnan(dur_val)) & (~np.isinf(dur_val)) & (evt_val >= 0)\n",
    "test_idx = (~np.isnan(dur_test)) & (~np.isinf(dur_test)) & (evt_test >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Fit CoxPH Model on Real Dataset\n",
    "# ----------------------------- #\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df[\"duration\"] = dur_train\n",
    "train_df[\"event\"] = evt_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test[test_idx])\n",
    "test_df[\"duration\"] = dur_test[test_idx]\n",
    "test_df[\"event\"] = evt_test[test_idx]\n",
    "\n",
    "cph_real = CoxPHFitter(penalizer=0.1)\n",
    "cph_real.fit(train_df, duration_col=\"duration\", event_col=\"event\")\n",
    "\n",
    "val_risk = cph_real.predict_partial_hazard(test_df)\n",
    "mean_cindex, lower, upper = bootstrap_cindex_ci(test_df[\"duration\"], test_df[\"event\"], val_risk)\n",
    "\n",
    "print(f\"Full Dataset C-index: {mean_cindex:.4f} (95% CI: {lower:.4f} â€“ {upper:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter setting \n",
    "\n",
    "num_syn=200  # Number of synthetic samples\n",
    "T = 20000    # Training Iterations   \n",
    "\n",
    "# DP parameters\n",
    "noise_multiplier =3 \n",
    "max_grad_norm = 1\n",
    "delta = 1e-5\n",
    "min_snr = 1                     # you can tune this\n",
    "min_noise_multiplier = 0.25      # ensures some privacy even if grad is small\n",
    "sample_rate=num_syn / len(X_train)\n",
    "accountant  = SelectiveAccountant(RDPAccountant(), max_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using to estimate zero-order gradients in Torch AutoDiff Setup\n",
    "class DPCoxGradientEstimator(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X_syn, use_dp=False, noise_multiplier=noise_multiplier, \n",
    "                clip_norm=max_grad_norm, epsilon_range=(0.25, 0.5), seed=None):\n",
    "        ctx.save_for_backward(X_syn)\n",
    "        ctx.use_dp = use_dp\n",
    "        ctx.noise_multiplier = noise_multiplier\n",
    "        ctx.clip_norm = clip_norm\n",
    "        ctx.epsilon_range = epsilon_range\n",
    "        ctx.seed = seed\n",
    "        X_np = X_syn.detach().cpu().numpy()\n",
    "        preds = cph_real.predict_log_partial_hazard(pd.DataFrame(X_np)).values.flatten() #scope of model is global, so avilable everywhere\n",
    "        return torch.tensor(preds, dtype=torch.float32, device=X_syn.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X_syn, = ctx.saved_tensors\n",
    "        X_np = X_syn.detach().cpu().numpy()\n",
    "        n, d = X_np.shape\n",
    "        rng = np.random.default_rng(ctx.seed)\n",
    "        \n",
    "        # Vectorized perturbation\n",
    "        epsilons = rng.uniform(low=ctx.epsilon_range[0], high=ctx.epsilon_range[1], size=d)\n",
    "        eye = np.eye(d)\n",
    "        X_plus = X_np[:, None, :] + epsilons * eye  # shape (n, d, d)\n",
    "        X_minus = X_np[:, None, :] - epsilons * eye  # shape (n, d, d)\n",
    "        \n",
    "        # Reshape for batch prediction\n",
    "        X_batch = np.vstack([X_plus.reshape(-1, d), X_minus.reshape(-1, d)])\n",
    "        hr_batch = cph_real.predict_log_partial_hazard(pd.DataFrame(X_batch)).values.flatten()\n",
    "        \n",
    "        # Reshape and compute gradients\n",
    "        hr_batch_plus = hr_batch[:n*d].reshape(n, d)\n",
    "        hr_batch_minus = hr_batch[n*d:].reshape(n, d)\n",
    "        hr_diff = (hr_batch_plus - hr_batch_minus)\n",
    "        grad_estimate = (hr_diff / (2 * epsilons)) * grad_output.cpu().numpy()[:, None]\n",
    "\n",
    "        # DP processing\n",
    "        if ctx.use_dp:\n",
    "            grad_norms = np.linalg.norm(grad_estimate, axis=1, keepdims=True)  # (n, 1)\n",
    "            clip_mask = grad_norms > ctx.clip_norm  # (n, 1)\n",
    "            \n",
    "            scaling = (ctx.clip_norm / (grad_norms + 1e-8))  # (n, 1)\n",
    "            grad_estimate = np.where(clip_mask, grad_estimate * scaling, grad_estimate)  # (n, d)\n",
    "\n",
    "            target_noise_std = np.mean(grad_norms) / min_snr\n",
    "\n",
    "            noise_options = np.array([0.25, 0.5, 0.75, 1.0, 1.5, 2.0]) * ctx.noise_multiplier * ctx.clip_norm\n",
    "            valid_noise = noise_options[noise_options >= target_noise_std]\n",
    "            if len(valid_noise) > 0:\n",
    "                chosen_noise_std = max(valid_noise.min(), ctx.clip_norm * min_noise_multiplier)\n",
    "            else:\n",
    "                chosen_noise_std = max(noise_options.max(), ctx.clip_norm * min_noise_multiplier)\n",
    "\n",
    "            noise = rng.normal(0, chosen_noise_std, grad_estimate.shape)\n",
    "            grad_estimate += noise\n",
    "\n",
    "            effective_noise_multiplier = chosen_noise_std / ctx.clip_norm\n",
    "            accountant.step(noise_multiplier=effective_noise_multiplier, sample_rate=sample_rate)\n",
    "\n",
    "\n",
    "        return torch.tensor(grad_estimate, dtype=torch.float32, device=X_syn.device), None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Initialising synthetic dataset\n",
    "# ================================================\n",
    "num_features = X_train.shape[1]\n",
    "num_bins = 20  # <-- change this to 4 for quartiles, 10 for deciles, etc.\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "t_min = dur_train.min()\n",
    "cutt = dur_train[evt_train == 0].max()\n",
    "\n",
    "bin_edges = np.linspace(t_min, cutt, num_bins + 1)\n",
    "\n",
    "# Allocate event samples evenly across bins\n",
    "samples_per_bin = (num_syn // 2) // num_bins\n",
    "\n",
    "T_events = [np.random.uniform(bin_edges[i], bin_edges[i + 1], samples_per_bin) for i in range(num_bins)]\n",
    "T_events = np.concatenate(T_events)\n",
    "T_events = np.pad(T_events, (0, max(0, (num_syn // 2) - len(T_events))), constant_values=cutt)\n",
    "T_censored = np.full((num_syn // 2,), cutt, dtype=np.float32)\n",
    "\n",
    "T_syn = np.concatenate([T_events, T_censored])\n",
    "E_syn = np.concatenate([np.ones_like(T_events), np.zeros_like(T_censored)])\n",
    "\n",
    "T_syn_tensor = torch.tensor(T_syn, dtype=torch.float32)\n",
    "E_syn_tensor = torch.tensor(E_syn, dtype=torch.float32)\n",
    "\n",
    "# Initialize X_syn with DP-noised stats, can be intialised randomly\n",
    "mu = X_train_tensor.mean(dim=0) + torch.normal(0, 0.001, size=(num_features,))\n",
    "std = X_train_tensor.std(dim=0) + torch.normal(0, 0.001, size=(num_features,)) + 1e-5\n",
    "X_syn = mu + torch.randn((num_syn, num_features), dtype=torch.float32) * std\n",
    "\n",
    "# setting grad true as we are going to update it\n",
    "X_syn = X_syn.requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Coverting to torch tensors and setting up optimiser\n",
    "# ----------------------------- #\n",
    "optimizer = torch.optim.Adam([X_syn], lr=0.01)\n",
    "dur_tensor = torch.tensor(dur_train, dtype=torch.float32)\n",
    "evt_tensor = torch.tensor(evt_train, dtype=torch.float32)\n",
    "indices_evt = (evt_tensor == 1).nonzero(as_tuple=True)[0]\n",
    "indices_cens = (evt_tensor == 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "### Computing Trained model preds for real dataset, used in matching loss\n",
    "X_real_sample = torch.tensor(X_train, dtype=torch.float32)\n",
    "pred_real = DPCoxGradientEstimator.apply(X_real_sample, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS1 = LOSS2 = LOSS = 0\n",
    "\n",
    "eval_itr = 100 # Validation after eval_itr iterations\n",
    "best_cindex = 0\n",
    "best_X = None\n",
    "\n",
    "for step in range(T):\n",
    "    \n",
    "    step_rng = torch.Generator().manual_seed(SEED + step)    \n",
    "    pred_syn =  DPCoxGradientEstimator.apply(X_syn, True, noise_multiplier, max_grad_norm, (0.25, 3),SEED+step) # Epsilon range tuned manually\n",
    "    \n",
    "    # Supervision Loss\n",
    "    loss_surv = coxph_loss(pred_syn, T_syn_tensor, E_syn_tensor)\n",
    "\n",
    "    # Distrubtion matching in preds space\n",
    "    pred_syn_cat,q1,q2=stratify_syn_pred(T_syn_tensor,E_syn_tensor,pred_syn)\n",
    "    pred_real_cat=stratify_real_pred(dur_tensor,indices_evt,indices_cens,q1,q2,X_syn,pred_real,step_rng)\n",
    "    loss_matching = torch.norm( pred_syn_cat - pred_real_cat, p=1) / (X_syn.shape[0] + 1e-8)\n",
    "\n",
    "    LOSS1 += loss_surv.item()\n",
    "    LOSS2 += loss_matching.item()\n",
    "\n",
    "    # Weighted combination of survival and gradient matching loss\n",
    "    desired_ratio = 0.1              # to be tuned manually\n",
    "    alpha = (loss_surv.item() / (loss_matching.item() + 1e-9)) * (desired_ratio / (1 - desired_ratio))\n",
    "    loss = loss_surv + alpha * loss_matching\n",
    "    \n",
    "    LOSS += loss.item()\n",
    "\n",
    "    # Backprop with DP update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    if step % eval_itr == 0:\n",
    "        c_index=evaluate_coxph(X_syn,T_syn,E_syn,dur_val,evt_val,X_val) \n",
    "        print(f\"Step {step:04d} | Total Loss: {LOSS/eval_itr:.3f} | Surv Loss: {LOSS1/eval_itr:.3f} | Grad Loss: {LOSS2/eval_itr:.3f} | C-index: {c_index:.4f}\")\n",
    "        LOSS = LOSS1 = LOSS2 = 0\n",
    "        if c_index > best_cindex:\n",
    "            best_cindex = c_index\n",
    "            best_X = X_syn.detach().clone()\n",
    "            best_step = step\n",
    "            accountant.max_steps = best_step \n",
    "        \n",
    "        if (step-best_step)>3000:  # Early stopping\n",
    "            break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing privacy budget\n",
    "eps_best, _ = accountant.get_privacy_spent(delta)\n",
    "print(f\"ðŸ“Š Final Privacy Budget (up to best step {best_step}): Îµ = {eps_best:.3f}, Î´ = {delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# Evaluate Condensed Dataset\n",
    "# ----------------------------- #\n",
    "train_df = pd.DataFrame(best_X)\n",
    "train_df[\"duration\"] = T_syn\n",
    "train_df[\"event\"] = E_syn\n",
    "\n",
    "cph_syn = CoxPHFitter(penalizer=0.1)\n",
    "cph_syn.fit(train_df, duration_col=\"duration\", event_col=\"event\")\n",
    "val_risk = cph_syn.predict_partial_hazard(test_df)\n",
    "\n",
    "mean_cindex, lower, upper = bootstrap_cindex_ci(test_df[\"duration\"], test_df[\"event\"], val_risk)\n",
    "print(f\"Condensed Model C-index: {mean_cindex:.4f} (95% CI: {lower:.4f} â€“ {upper:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict = {\n",
    "#     'data': best_X,\n",
    "#     'time': T_syn,\n",
    "#     'event':E_syn\n",
    "# }\n",
    "# import pickle\n",
    "\n",
    "# with open('./condensed/COX_SEER_'+str(int(num_syn/2))+'.pkl', 'wb') as f:\n",
    "#     pickle.dump(dataset_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
