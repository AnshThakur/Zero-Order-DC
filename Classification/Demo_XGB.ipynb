{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, \n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from opacus.accountants import RDPAccountant\n",
    "\n",
    "from loaders_data import process_clinical_data\n",
    "from utils import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "SEED = 40\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Load and Prepare Data\n",
    "# ----------------------\n",
    "TRAIN, VAL, TEST = process_clinical_data('./final_data/UHB.csv')\n",
    "X_train, y_train = TRAIN\n",
    "X_test, y_test = TEST\n",
    "X_val, y_val = VAL\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Train XGBoost Model\n",
    "# ----------------------\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 3,\n",
    "    \"subsample\": 0.7,\n",
    "    \"reg_lambda\": 2.0,\n",
    "    \"reg_alpha\": 1.0,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"eval_metric\": \"auc\"\n",
    "}\n",
    "\n",
    "print(\"\\n=== Training XGBoost Binary Classifier ===\")\n",
    "xgb_model = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    evals=[(dval, \"validation\")],\n",
    "    num_boost_round=15000,\n",
    "    early_stopping_rounds=250,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Initial Evaluation\n",
    "# ----------------------\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "loss = binary_cross_entropy(y_test, y_pred)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "svc_pr_auc = auc(recall, precision)\n",
    "\n",
    "# Evaluate on separate validation and test sets\n",
    "print(\"\\n=== Evaluation with Threshold Optimization ===\")\n",
    "evaluator = BinaryClassifierEvaluator()\n",
    "\n",
    "results = evaluator.evaluate_separately(\n",
    "    y_val_true=y_val,\n",
    "    y_val_prob=xgb_model.predict(dval),\n",
    "    y_test_true=y_test,\n",
    "    y_test_prob=y_pred,\n",
    "    target_metric='Recall',\n",
    "    target_value=0.85,\n",
    "    error_margin=0.1,\n",
    "    n_bootstraps=1000,return_ci=True\n",
    ")\n",
    "print(\" | \".join([f\"FULL DATASET---\"] + [f\"{k}: {v}\" for k, v in results.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Initialize DP Settings, to be tuned for required privacy budget and condensed samples\n",
    "# ----------------------\n",
    "num_syn = 200 # condensed samples\n",
    "T = 30000\n",
    "delta = 1e-5\n",
    "noise_multiplier = 1\n",
    "max_grad_norm = 1\n",
    "min_snr = 1.25\n",
    "min_noise_multiplier = 0.25\n",
    "sample_rate = num_syn / len(X_train)\n",
    "accountant = SelectiveAccountant(RDPAccountant(), max_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPFiniteDiffGradientEstimator(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X_syn, use_dp=False, noise_multiplier=noise_multiplier, clip_norm=max_grad_norm, \n",
    "                epsilon_range=(0.25, 0.5), seed=None):\n",
    "        ctx.save_for_backward(X_syn)\n",
    "        ctx.use_dp = use_dp\n",
    "        ctx.noise_multiplier = noise_multiplier\n",
    "        ctx.clip_norm = clip_norm\n",
    "        ctx.epsilon_range = epsilon_range\n",
    "        ctx.seed = seed\n",
    "        ctx.model = xgb_model\n",
    "\n",
    "        X_np = X_syn.detach().cpu().numpy()\n",
    "        preds = ctx.model.predict(xgb.DMatrix(X_np))\n",
    "        return torch.tensor(preds, dtype=torch.float32, device=X_syn.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        X_syn, = ctx.saved_tensors\n",
    "        device = X_syn.device\n",
    "        model = ctx.model\n",
    "        seed = ctx.seed\n",
    "        eps_range = ctx.epsilon_range\n",
    "        use_dp = ctx.use_dp\n",
    "        noise_multiplier = ctx.noise_multiplier\n",
    "        clip_norm = ctx.clip_norm\n",
    "\n",
    "        X_np = X_syn.detach().cpu().numpy()\n",
    "        n, d = X_np.shape\n",
    "        rng = np.random.default_rng(seed)\n",
    "        epsilons = rng.uniform(low=eps_range[0], high=eps_range[1], size=(d,))\n",
    "        grad_estimate = np.zeros_like(X_np)\n",
    "\n",
    "        # Perturbation and prediction (batched)\n",
    "        X_batch = []\n",
    "        for j in range(d):\n",
    "            X_plus = X_np.copy(); X_plus[:, j] += epsilons[j]\n",
    "            X_minus = X_np.copy(); X_minus[:, j] -= epsilons[j]\n",
    "            X_batch.append(X_plus)\n",
    "            X_batch.append(X_minus)\n",
    "\n",
    "        X_batch = np.concatenate(X_batch, axis=0)\n",
    "        preds_batch = model.predict(xgb.DMatrix(X_batch))  # shape: (2 * d * n,)\n",
    "        preds_batch = preds_batch.reshape(2 * d, n)\n",
    "  \n",
    "        # Compute finite-diff gradients\n",
    "        for j in range(d):\n",
    "            f_plus = preds_batch[2 * j]\n",
    "            f_minus = preds_batch[2 * j + 1]\n",
    "            df_dx = (f_plus - f_minus) / (2 * epsilons[j])\n",
    "            grad_estimate[:, j] = df_dx * grad_output.detach().cpu().numpy()\n",
    "\n",
    "        # DP noise addition\n",
    "        if use_dp:\n",
    "            preclip_norm = np.linalg.norm(grad_estimate)\n",
    "            # DP clipping\n",
    "            if preclip_norm > clip_norm:\n",
    "               grad_estimate *= clip_norm / (preclip_norm + 1e-6)\n",
    "               grad_norm = clip_norm\n",
    "            else:\n",
    "               grad_norm = preclip_norm  # carry through unclipped value\n",
    "            \n",
    "            min_noise_scale = clip_norm * (noise_multiplier * min_noise_multiplier)  # Minimum protection\n",
    "            target_noise_std = grad_norm / min_snr\n",
    "            # Discrete noise levels (multiples of base noise_multiplier)\n",
    "            noise_options = np.array([0.25, 0.5, 0.75, 1.0, 1.5, 2.0]) * noise_multiplier * clip_norm\n",
    "            chosen_noise_std = max(min(noise_options[noise_options >= target_noise_std]), min_noise_scale) if len(noise_options[noise_options >= target_noise_std]) > 0 else min_noise_scale\n",
    "\n",
    "            valid_noise = noise_options[noise_options >= target_noise_std]\n",
    "            \n",
    "            if len(valid_noise) > 0:\n",
    "                # Case 1: Can satisfy SNR exactly\n",
    "                chosen_noise_std = max(valid_noise.min(), min_noise_scale)\n",
    "            else:\n",
    "                # Case 2: Use largest available noise while respecting minimum\n",
    "                chosen_noise_std = max(noise_options.max(), min_noise_scale)\n",
    "\n",
    "            noise = rng.normal(0, chosen_noise_std, grad_estimate.shape)\n",
    "            grad_estimate = grad_estimate + noise\n",
    "\n",
    "            effective_noise_multiplier = chosen_noise_std / clip_norm\n",
    "            accountant.step(noise_multiplier=effective_noise_multiplier,sample_rate=sample_rate)\n",
    "            # print(f\"[DP-Step] pre-clip={preclip_norm:.3f}, final={grad_norm:.3f}, noise_std={chosen_noise_std:.3f}, SNR={grad_norm / chosen_noise_std:.2f}\"\n",
    "        return torch.tensor(grad_estimate, dtype=torch.float32, device=device), None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Initialize Synthetic Dataset, can be done based on mean+noise of real classes \n",
    "# ================================================\n",
    "# num_features = X_train.shape[1]\n",
    "# X_syn = torch.randn((num_syn, num_features), dtype=torch.float32, requires_grad=True)\n",
    "# y_syn = torch.cat([torch.zeros(num_syn//2), torch.ones(num_syn//2)])\n",
    "# evaluate(X_syn,y_syn,xgb_params,dval,X_val,y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Synthetic Data Init\n",
    "# ----------------------\n",
    "num_features = X_train.shape[1]\n",
    "pos_ratio = 0.5\n",
    "dp_noise_scale = 0.01\n",
    "num_pos = int(num_syn * pos_ratio)\n",
    "num_neg = num_syn - num_pos\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_train_0 = X_train_tensor[y_train_tensor == 0]\n",
    "X_train_1 = X_train_tensor[y_train_tensor == 1]\n",
    "\n",
    "mu_0 = X_train_0.mean(dim=0) + torch.normal(0, dp_noise_scale, size=(num_features,))\n",
    "std_0 = X_train_0.std(dim=0) + torch.normal(0, dp_noise_scale, size=(num_features,)) + 1e-5\n",
    "mu_1 = X_train_1.mean(dim=0) + torch.normal(0, dp_noise_scale, size=(num_features,))\n",
    "std_1 = X_train_1.std(dim=0) + torch.normal(0, dp_noise_scale, size=(num_features,)) + 1e-5\n",
    "\n",
    "X_syn_0 = mu_0 + torch.randn((num_neg, num_features)) * std_0\n",
    "X_syn_1 = mu_1 + torch.randn((num_pos, num_features)) * std_1\n",
    "X_syn = torch.cat([X_syn_0, X_syn_1], dim=0).requires_grad_(True)\n",
    "y_syn = torch.cat([torch.zeros(num_neg), torch.ones(num_pos)])\n",
    "\n",
    "evaluate(X_syn, y_syn, xgb_params, dval, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting preds from model for real examples, to be used in matching loss\n",
    "pred_real = DPFiniteDiffGradientEstimator.apply(torch.tensor(X_train, dtype=torch.float32), SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS1 = LOSS2 = LOSS = 0\n",
    "eval_itr = 100\n",
    "best_cindex = 0\n",
    "best_X = None\n",
    "optimizer = torch.optim.Adam([X_syn], lr=0.01)\n",
    "\n",
    "for step in range(T):\n",
    "    \n",
    "    step_rng = torch.Generator().manual_seed(SEED + step)    \n",
    "    pred_syn =  DPFiniteDiffGradientEstimator.apply(X_syn, True, noise_multiplier, max_grad_norm, (0.25, 3),SEED+step)\n",
    "    \n",
    "    # Supervision Loss\n",
    "    loss_pred = torch.nn.BCELoss()(pred_syn, y_syn)\n",
    "\n",
    "    # Distrubtion matching in preds space\n",
    "    pred_syn_cat, idx_groups = stratify_pred_binary(y_syn, pred_syn)\n",
    "    pred_real_cat = stratify_real_pred_binary(pred_real.clone().detach(), idx_groups, X_syn, step_rng)\n",
    "    loss_matching = torch.norm(pred_syn_cat - pred_real_cat, p=1) / (X_syn.shape[0] + 1e-8)\n",
    "\n",
    "    # Weighted combination of survival and gradient matching loss\n",
    "    desired_ratio = 0.01 # hyperparameter\n",
    "    alpha = (loss_pred.item() / (loss_matching.item() + 1e-9)) * (desired_ratio / (1 - desired_ratio))\n",
    "    loss = loss_pred + alpha * loss_matching\n",
    "\n",
    "    LOSS1 += loss_pred.item()\n",
    "    LOSS2 += alpha*loss_matching.item()\n",
    "    LOSS += loss.item()\n",
    "\n",
    "    # Backprop with DP update\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    if step>0 and step % eval_itr == 0:\n",
    "        c_index=evaluate(X_syn,y_syn,xgb_params,dval,X_val,y_val) \n",
    "        print(f\"Step {step:04d} | Total Loss: {LOSS/eval_itr:.3f} | Pred Loss: {LOSS1/eval_itr:.3f} | Grad Loss: {LOSS2/eval_itr:.3f} | AUROC: {c_index:.4f}\")\n",
    "        LOSS = LOSS1 = LOSS2 = 0\n",
    "        if c_index > best_cindex:\n",
    "            best_cindex = c_index\n",
    "            best_X = X_syn.detach().clone()\n",
    "            best_step = step\n",
    "            accountant.max_steps = best_step \n",
    "        # Early stopping\n",
    "        if (step-best_step)>4000:\n",
    "            break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_best, _ = accountant.get_privacy_spent(delta)\n",
    "print(f\"📊 Final Privacy Budget (up to best step {best_step}): ε = {eps_best:.3f}, δ = {delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_syn = xgb.DMatrix(data=best_X, label=y_syn.detach().numpy())\n",
    "model_eval = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=d_syn,\n",
    "    evals=[(dval, \"validation\")],\n",
    "    num_boost_round=5000,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n================ FINAL METRICS ================\")\n",
    "y_pred = model_eval.predict(dtest)\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "loss = binary_cross_entropy(y_test, y_pred)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "svc_pr_auc = auc(recall, precision)\n",
    "print(f\"AUROC: {auroc:.4f} | AUPRC: {svc_pr_auc:.4f} | CE Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
